{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffbbf9a",
   "metadata": {},
   "source": [
    "# RiskSampler – Intelligent Sample Weighting for XGBoost  \n",
    "Demo notebook showcasing multiple weighting strategies available in **RiskSampler** and their impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9abbc0",
   "metadata": {},
   "source": [
    "## Objectives  \n",
    "1. Load and clean a **binary classification** dataset (Lending Club Loans).  \n",
    "2. Create different sample–weight schemes with **RiskSampler** (`balanced`, `equal_vintage`, `stabilise_er`, `recency_decay`, `expected_loss`, `combo`).  \n",
    "3. Train an **XGBoost** classifier **with and without** sample weights.  \n",
    "4. Compare metrics (MCC, ROC‑AUC, Precision, Recall, F1).  \n",
    "5. Provide a template you can adapt to your own datasets and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a20ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line the first time you run the notebook\n",
    "# %pip install -e ..   # path to RiskSampler repo (edit as needed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings, os, pathlib, sys, datetime as dt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (matthews_corrcoef, roc_auc_score,\n",
    "                             precision_score, recall_score, f1_score)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# RiskSampler – adjust import if your package layout differs\n",
    "# Example API: SampleWeightFactory(strategy).fit_transform(...)\n",
    "from risk_sampler import RiskSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77c0a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Configuration parameters #\n",
    "############################\n",
    "DATA_PATH = pathlib.Path('../../datasets/lending_club/accepted_2007_to_2018Q4.csv')\n",
    "NROWS     = 200_000           # adjust for faster runs\n",
    "TARGET_RAW = 'loan_status'\n",
    "TARGET     = 'target'\n",
    "DATE_COL   = 'issue_d'        # YYYY‑MM formatted (in LendingClub)\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE   = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48d096e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (200000, 151) rows × 151 cols\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>hardship_payoff_balance_amount</th>\n",
       "      <th>hardship_last_payment_amount</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_amount</th>\n",
       "      <th>settlement_percentage</th>\n",
       "      <th>settlement_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68407277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.99</td>\n",
       "      <td>123.03</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68355089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.99</td>\n",
       "      <td>820.28</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68341763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>10.78</td>\n",
       "      <td>432.66</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66310712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>14.85</td>\n",
       "      <td>829.90</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68476807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>22.45</td>\n",
       "      <td>289.91</td>\n",
       "      <td>F</td>\n",
       "      <td>F1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  68407277        NaN     3600.0       3600.0           3600.0   36 months   \n",
       "1  68355089        NaN    24700.0      24700.0          24700.0   36 months   \n",
       "2  68341763        NaN    20000.0      20000.0          20000.0   60 months   \n",
       "3  66310712        NaN    35000.0      35000.0          35000.0   60 months   \n",
       "4  68476807        NaN    10400.0      10400.0          10400.0   60 months   \n",
       "\n",
       "   int_rate  installment grade sub_grade  ... hardship_payoff_balance_amount  \\\n",
       "0     13.99       123.03     C        C4  ...                            NaN   \n",
       "1     11.99       820.28     C        C1  ...                            NaN   \n",
       "2     10.78       432.66     B        B4  ...                            NaN   \n",
       "3     14.85       829.90     C        C5  ...                            NaN   \n",
       "4     22.45       289.91     F        F1  ...                            NaN   \n",
       "\n",
       "  hardship_last_payment_amount disbursement_method  debt_settlement_flag  \\\n",
       "0                          NaN                Cash                     N   \n",
       "1                          NaN                Cash                     N   \n",
       "2                          NaN                Cash                     N   \n",
       "3                          NaN                Cash                     N   \n",
       "4                          NaN                Cash                     N   \n",
       "\n",
       "  debt_settlement_flag_date settlement_status settlement_date  \\\n",
       "0                       NaN               NaN             NaN   \n",
       "1                       NaN               NaN             NaN   \n",
       "2                       NaN               NaN             NaN   \n",
       "3                       NaN               NaN             NaN   \n",
       "4                       NaN               NaN             NaN   \n",
       "\n",
       "  settlement_amount settlement_percentage settlement_term  \n",
       "0               NaN                   NaN             NaN  \n",
       "1               NaN                   NaN             NaN  \n",
       "2               NaN                   NaN             NaN  \n",
       "3               NaN                   NaN             NaN  \n",
       "4               NaN                   NaN             NaN  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataset(path: pathlib.Path, nrows=None):\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path, nrows=nrows, low_memory=False)\n",
    "        print(f'Loaded {df.shape} rows × {df.shape[1]} cols')\n",
    "        return df\n",
    "    else:\n",
    "        print(f'⚠️ Dataset not found at {path}. Generating synthetic demo dataset…')\n",
    "        from sklearn.datasets import make_classification\n",
    "        X, y = make_classification(n_samples=20_000, n_features=30,\n",
    "                                   n_informative=10, n_redundant=5,\n",
    "                                   weights=[0.88, 0.12],\n",
    "                                   random_state=RANDOM_SEED)\n",
    "        df = pd.DataFrame(X, columns=[f'feat_{i}' for i in range(X.shape[1])])\n",
    "        df['issue_d'] = (pd.Timestamp('2010‑01‑01') + \n",
    "                         pd.to_timedelta(np.random.randint(0, 365*10, size=len(df)), unit='D'))\n",
    "        df[TARGET_RAW] = np.where(y==1, 'Charged Off', 'Fully Paid')\n",
    "        return df\n",
    "\n",
    "df = load_dataset(DATA_PATH, NROWS)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fad98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    0.824545\n",
      "1    0.175455\n",
      "Name: event_rate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Map raw loan_status to binary target\n",
    "positive_statuses = {'Charged Off', 'Default', 'Does not meet the credit policy. Status:Charged Off'}\n",
    "df[TARGET] = df[TARGET_RAW].isin(positive_statuses).astype(int)\n",
    "\n",
    "print(df[TARGET].value_counts(normalize=True).rename('event_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba37e4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 113 numeric features\n"
     ]
    }
   ],
   "source": [
    "# Very naive feature subset for demo – drop high cardinality or leakage columns\n",
    "drop_cols = [TARGET_RAW, TARGET, DATE_COL, 'id', 'member_id']\n",
    "num_cols  = df.select_dtypes(include='number').columns.difference(drop_cols)\n",
    "cat_cols  = df.select_dtypes(include='object').columns.difference(drop_cols + [DATE_COL])\n",
    "\n",
    "# For simplicity, use only numeric columns\n",
    "features = num_cols.tolist()\n",
    "print(f'Selected {len(features)} numeric features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c0dbaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (150000, 113) | Pos rate: 0.1755\n",
      "Test  shape: (50000, 113) | Pos rate: 0.1755\n"
     ]
    }
   ],
   "source": [
    "X = df[features]\n",
    "y = df[TARGET]\n",
    "\n",
    "# Simple YYYY‑MM vintage\n",
    "\n",
    "df['vintage'] = pd.to_datetime(df[DATE_COL]).dt.to_period('M').astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test, vint_train, vint_test = train_test_split(\n",
    "    X, y, df['vintage'], test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED)\n",
    "\n",
    "print('Train shape:', X_train.shape, '| Pos rate:', y_train.mean().round(4))\n",
    "print('Test  shape:', X_test.shape, '| Pos rate:', y_test.mean().round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b604c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_tr, y_tr, X_te, y_te):\n",
    "    yhat_tr = model.predict(X_tr)\n",
    "    yhat_te = model.predict(X_te)\n",
    "\n",
    "    metrics = {\n",
    "        'MCC_train': matthews_corrcoef(y_tr, yhat_tr),\n",
    "        'MCC_test' : matthews_corrcoef(y_te, yhat_te),\n",
    "        'ROC_AUC_train': roc_auc_score(y_tr, model.predict_proba(X_tr)[:,1]),\n",
    "        'ROC_AUC_test' : roc_auc_score(y_te, model.predict_proba(X_te)[:,1]),\n",
    "        'F1_test'  : f1_score(y_te, yhat_te),\n",
    "        'Precision_test': precision_score(y_te, yhat_te),\n",
    "        'Recall_test'   : recall_score(y_te, yhat_te)\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97d15ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (no weights):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MCC_train': 0.9940033791927688,\n",
       " 'MCC_test': 0.9919021976135908,\n",
       " 'ROC_AUC_train': 0.9999372104294799,\n",
       " 'ROC_AUC_test': 0.9997002331902716,\n",
       " 'F1_test': 0.9932885906040269,\n",
       " 'Precision_test': 0.9997690531177829,\n",
       " 'Recall_test': 0.9868915992248946}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline_metrics = evaluate(baseline, X_train, y_train, X_test, y_test)\n",
    "print('Baseline (no weights):')\n",
    "baseline_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b28249f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BALANCED ===\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['2015-11', '2015-10', '2015-12', '2015-09', '2015-08', '2015-10',\\n       '2015-11', '2015-08', '2015-12', '2015-10',\\n       ...\\n       '2015-10', '2015-11', '2015-10', '2015-12', '2015-09', '2015-12',\\n       '2015-09', '2015-12', '2015-09', '2015-12'],\\n      dtype='object', length=150000)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     61\u001b[39m factory = RiskSampler(**params)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m sw = \u001b[43mfactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# adjust API if different\u001b[39;00m\n\u001b[32m     64\u001b[39m model = XGBClassifier(\n\u001b[32m     65\u001b[39m     n_estimators=\u001b[32m300\u001b[39m,\n\u001b[32m     66\u001b[39m     max_depth=\u001b[32m4\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     74\u001b[39m )\n\u001b[32m     75\u001b[39m model.fit(X_train, y_train, sample_weight=sw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\0_CienciaDados\\1_Frameworks\\RiskSampler\\src\\risk_sampler\\core.py:170\u001b[39m, in \u001b[36mRiskSampler.fit_transform\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pd.DataFrame) -> pd.Series:  \u001b[38;5;66;03m# noqa: D401\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Shortcut for *fit* followed by *transform*.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m.transform(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\0_CienciaDados\\1_Frameworks\\RiskSampler\\src\\risk_sampler\\core.py:101\u001b[39m, in \u001b[36mRiskSampler.fit\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute dataset‑level statistics required by some strategies.\"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m df = df.copy()\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33m_vintage\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._to_period(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdate_col\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# basic stats\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.summary_ = {\n\u001b[32m    105\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_obs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df),\n\u001b[32m    106\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mglobal_er\u001b[39m\u001b[33m\"\u001b[39m: df[\u001b[38;5;28mself\u001b[39m.target_col].mean(),\n\u001b[32m    107\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvintage_sizes\u001b[39m\u001b[33m\"\u001b[39m: df[\u001b[33m\"\u001b[39m\u001b[33m_vintage\u001b[39m\u001b[33m\"\u001b[39m].value_counts().to_dict(),\n\u001b[32m    108\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvintage_er\u001b[39m\u001b[33m\"\u001b[39m: df.groupby(\u001b[33m\"\u001b[39m\u001b[33m_vintage\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[38;5;28mself\u001b[39m.target_col].mean().to_dict(),\n\u001b[32m    109\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JM\\envs\\risksampler-lock\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JM\\envs\\risksampler-lock\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JM\\envs\\risksampler-lock\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['2015-11', '2015-10', '2015-12', '2015-09', '2015-08', '2015-10',\\n       '2015-11', '2015-08', '2015-12', '2015-10',\\n       ...\\n       '2015-10', '2015-11', '2015-10', '2015-12', '2015-09', '2015-12',\\n       '2015-09', '2015-12', '2015-09', '2015-12'],\\n      dtype='object', length=150000)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "schemes = {\n",
    "    'balanced': {\n",
    "        'strategies': 'balanced',\n",
    "        'date_col': vint_train,\n",
    "        'target_col': y_train,\n",
    "    },\n",
    "    'equal_vintage': {\n",
    "        'strategies': 'equal_vintage',\n",
    "        'vintage_col': vint_train,\n",
    "        'target_col': y_train,\n",
    "        'date_col': vint_train,\n",
    "    },\n",
    "    'stabilise_er': {\n",
    "        'strategies': 'stabilise_er',\n",
    "        'target_col': y_train,\n",
    "        'date_col': vint_train,\n",
    "    },\n",
    "    'recency_decay': {\n",
    "        'strategies': 'recency_decay',\n",
    "        'vintage_col': vint_train,\n",
    "        'decay_rate': 0.1,\n",
    "        'target_col': y_train,\n",
    "        'date_col': vint_train,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add expected_loss only if column exists\n",
    "if 'loan_amnt' in df.columns:\n",
    "    schemes['expected_loss'] = {\n",
    "        'strategies': 'expected_loss',\n",
    "        'loss_col': X_train['loan_amnt'],\n",
    "        'target_col': y_train,\n",
    "        'date_col': vint_train,\n",
    "    }\n",
    "    schemes['combo'] = {\n",
    "        'strategies': 'combo',\n",
    "        'components': [\n",
    "            {\n",
    "                'strategies': 'expected_loss',\n",
    "                'loss_col': X_train['loan_amnt'],\n",
    "                'target_col': y_train,\n",
    "                'date_col': vint_train,\n",
    "            },\n",
    "            {\n",
    "                'strategies': 'recency_decay',\n",
    "                'vintage_col': vint_train,\n",
    "                'decay_rate': 0.1,\n",
    "                'target_col': y_train,\n",
    "                'date_col': vint_train,\n",
    "            }\n",
    "        ],\n",
    "        'target_col': y_train,\n",
    "        'date_col': vint_train,\n",
    "    }\n",
    "\n",
    "\n",
    "models_metrics = {}\n",
    "\n",
    "for name, params in schemes.items():\n",
    "    print(f'\\n=== {name.upper()} ===')\n",
    "    factory = RiskSampler(**params)\n",
    "    sw = factory.fit_transform(X_train)   # adjust API if different\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train, sample_weight=sw)\n",
    "    metrics = evaluate(model, X_train, y_train, X_test, y_test)\n",
    "    models_metrics[name] = metrics\n",
    "\n",
    "models_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df = pd.DataFrame(models_metrics).T\n",
    "results_df.loc['baseline'] = baseline_metrics\n",
    "results_df = results_df.sort_values('MCC_test', ascending=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_df['MCC_test'].plot(kind='barh')\n",
    "plt.title('MCC on test set by weighting scheme')\n",
    "plt.xlabel('Matthews Corr. Coef')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45670f22",
   "metadata": {},
   "source": [
    "## Conclusions & Next Steps  \n",
    "* **RiskSampler** allows flexible weighting strategies that can substantially improve model performance on imbalanced, drifting, or cost‑sensitive datasets.  \n",
    "* In this demo the *best* scheme was the one that maximised **MCC** on the test set. You might choose a different metric aligned with your business objective.  \n",
    "* Try tweaking:\n",
    "  * hyper‑parameters of XGBoost,\n",
    "  * decay rates or target event‑rates inside each weighting scheme,\n",
    "  * combining more than two strategies in `combo`.  \n",
    "\n",
    "Replace the dataset with your own and adjust `TARGET_RAW`, `positive_statuses`, and `DATE_COL` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Re‑run RiskSampler schemes with corrected parameters ===\n",
    "from pprint import pprint\n",
    "\n",
    "schemes = {\n",
    "    'balanced': {\n",
    "        'strategies': 'balanced',\n",
    "        'date_col': vint_train,\n",
    "        'target_col': y_train,\n",
    "    },\n",
    "    'equal_vintage': {\n",
    "        'strategies': 'equal_vintage',\n",
    "        'vintage_col': vint_train,\n",
    "        'date_col': vint_train,\n",
    "        'target_col': y_train,\n",
    "    },\n",
    "    'stabilise_er': {\n",
    "        'strategies': 'stabilise_er',\n",
    "        'date_col': vint_train,\n",
    "        'target_col': y_train,\n",
    "    },\n",
    "    'recency_decay': {\n",
    "        'strategies': 'recency_decay',\n",
    "        'vintage_col': vint_train,\n",
    "        'decay_rate': 0.1,\n",
    "        'date_col': vint_train,\n",
    "        'target_col': y_train,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Adiciona expected_loss e combo se loan_amnt existir\n",
    "if 'loan_amnt' in X_train.columns:\n",
    "    schemes['expected_loss'] = {\n",
    "        'strategies': 'expected_loss',\n",
    "        'loss_col': X_train['loan_amnt'],\n",
    "        'date_col': vint_train,\n",
    "        'target_col': y_train,\n",
    "    }\n",
    "    schemes['combo'] = {\n",
    "        'strategies': 'combo',\n",
    "        'components': [\n",
    "            {\n",
    "                'strategies': 'expected_loss',\n",
    "                'loss_col': X_train['loan_amnt'],\n",
    "                'date_col': vint_train,\n",
    "                'target_col': y_train,\n",
    "            },\n",
    "            {\n",
    "                'strategies': 'recency_decay',\n",
    "                'vintage_col': vint_train,\n",
    "                'decay_rate': 0.1,\n",
    "                'date_col': vint_train,\n",
    "                'target_col': y_train,\n",
    "            }\n",
    "        ],\n",
    "        'date_col': vint_train,\n",
    "        'target_col': y_train,\n",
    "    }\n",
    "\n",
    "models_metrics = {}\n",
    "for name, params in schemes.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    rs = RiskSampler(**params)\n",
    "    sw = rs.fit_transform(X_train)\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train, sample_weight=sw)\n",
    "    metrics = evaluate(model, X_train, y_train, X_test, y_test)\n",
    "    models_metrics[name] = metrics\n",
    "\n",
    "pprint(models_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risksampler-lock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "modified": "2025-06-18T03:33:24.613034+00:00"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
